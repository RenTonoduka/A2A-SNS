"""
X Post Extractor - ãƒã‚¹ãƒˆæŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ç‰¹å®šã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦JSON/CSV/Markdownã§å‡ºåŠ›
"""

import asyncio
import json
import csv
import re
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict, field

from playwright.async_api import Page, Locator

from config import DATA_DIR, LOGS_DIR, ExtractorConfig
from session_manager import SessionManager, BrowserConfig

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOGS_DIR / "extractor.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class XPost:
    """Xãƒã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿"""
    post_id: str
    author_username: str
    author_name: str
    content: str
    timestamp: str
    likes: int = 0
    retweets: int = 0
    replies: int = 0
    views: int = 0
    is_retweet: bool = False
    is_reply: bool = False
    media_urls: List[str] = field(default_factory=list)
    post_url: str = ""
    extracted_at: str = field(default_factory=lambda: datetime.now().isoformat())


class PostExtractor:
    """
    Xãƒã‚¹ãƒˆæŠ½å‡ºã‚¯ãƒ©ã‚¹

    æ©Ÿèƒ½:
    1. ç‰¹å®šã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒã‚¹ãƒˆã‚’æŠ½å‡º
    2. ã„ã„ã­æ•°ãƒ»RTæ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    3. JSON/CSV/Markdownå½¢å¼ã§å‡ºåŠ›
    """

    def __init__(
        self,
        session_manager: SessionManager,
        config: Optional[ExtractorConfig] = None
    ):
        self.session = session_manager
        self.config = config or ExtractorConfig()
        self._extracted_posts: Dict[str, List[XPost]] = {}

    @property
    def page(self) -> Page:
        if not self.session.page:
            raise RuntimeError("Session not started")
        return self.session.page

    # ==========================================
    # ãƒã‚¹ãƒˆæŠ½å‡º
    # ==========================================

    async def extract_from_account(
        self,
        username: str,
        max_posts: Optional[int] = None
    ) -> List[XPost]:
        """
        ç‰¹å®šã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒã‚¹ãƒˆã‚’æŠ½å‡º

        Args:
            username: @ãªã—ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            max_posts: æœ€å¤§å–å¾—æ•°ï¼ˆNoneã®å ´åˆã¯configå€¤ï¼‰
        """
        max_posts = max_posts or self.config.max_posts_per_account
        logger.info(f"ğŸ“¥ Extracting posts from @{username} (max: {max_posts})")

        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ç§»å‹•
        profile_url = f"https://x.com/{username}"
        await self.page.goto(profile_url, wait_until="networkidle")
        await asyncio.sleep(2)

        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if await self._check_account_not_found():
            logger.warning(f"âš ï¸ Account @{username} not found")
            return []

        posts: List[XPost] = []
        seen_ids: set = set()
        scroll_attempts = 0
        no_new_posts_count = 0

        while len(posts) < max_posts and scroll_attempts < self.config.max_scroll_attempts:
            # ç¾åœ¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒã‚¹ãƒˆã‚’å–å¾—
            post_elements = await self.page.locator('article[data-testid="tweet"]').all()

            for element in post_elements:
                try:
                    post = await self._parse_post_element(element, username)

                    if post and post.post_id not in seen_ids:
                        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        if self._should_include_post(post):
                            posts.append(post)
                            seen_ids.add(post.post_id)
                            logger.debug(f"  âœ… Extracted: {post.content[:50]}...")

                            if len(posts) >= max_posts:
                                break

                except Exception as e:
                    logger.debug(f"  âš ï¸ Failed to parse post: {e}")
                    continue

            # æ–°ã—ã„ãƒã‚¹ãƒˆãŒãªã‘ã‚Œã°ã‚«ã‚¦ãƒ³ãƒˆ
            if len(posts) == len(seen_ids) - len(posts):
                no_new_posts_count += 1
                if no_new_posts_count >= 5:
                    logger.info("ğŸ“­ No more new posts found")
                    break
            else:
                no_new_posts_count = 0

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
            await self._scroll_down()
            scroll_attempts += 1

            logger.info(f"  ğŸ“Š Progress: {len(posts)}/{max_posts} posts")

        logger.info(f"âœ… Extracted {len(posts)} posts from @{username}")
        self._extracted_posts[username] = posts
        return posts

    async def _parse_post_element(
        self,
        element: Locator,
        target_username: str
    ) -> Optional[XPost]:
        """ãƒã‚¹ãƒˆè¦ç´ ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            # ãƒã‚¹ãƒˆIDå–å¾—ï¼ˆURLã‹ã‚‰ï¼‰
            post_link = element.locator('a[href*="/status/"]').first
            href = await post_link.get_attribute("href") if await post_link.count() > 0 else None

            if not href:
                return None

            # URLã‹ã‚‰ãƒã‚¹ãƒˆIDã‚’æŠ½å‡º
            match = re.search(r'/status/(\d+)', href)
            if not match:
                return None

            post_id = match.group(1)

            # è‘—è€…æƒ…å ±
            author_link = element.locator('a[role="link"]').first
            author_href = await author_link.get_attribute("href") if await author_link.count() > 0 else ""
            author_username = author_href.strip("/") if author_href else target_username

            # è‘—è€…å
            author_name_elem = element.locator('[data-testid="User-Name"]').first
            author_name = await author_name_elem.inner_text() if await author_name_elem.count() > 0 else ""
            # æ”¹è¡Œã§åˆ†å‰²ã—ã¦æœ€åˆã®è¡Œã‚’å–å¾—
            author_name = author_name.split("\n")[0] if author_name else author_username

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            content_elem = element.locator('[data-testid="tweetText"]').first
            content = await content_elem.inner_text() if await content_elem.count() > 0 else ""

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            time_elem = element.locator('time').first
            timestamp = await time_elem.get_attribute("datetime") if await time_elem.count() > 0 else ""

            # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ•°
            likes = await self._extract_engagement(element, "like")
            retweets = await self._extract_engagement(element, "retweet")
            replies = await self._extract_engagement(element, "reply")

            # ãƒ“ãƒ¥ãƒ¼æ•°ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            views = 0
            views_elem = element.locator('a[href*="/analytics"]').first
            if await views_elem.count() > 0:
                views_text = await views_elem.inner_text()
                views = self._parse_number(views_text)

            # ãƒªãƒ„ã‚¤ãƒ¼ãƒˆãƒ»ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š
            is_retweet = "Reposted" in author_name or "ãŒãƒªãƒã‚¹ãƒˆ" in author_name
            is_reply = await element.locator('[data-testid="socialContext"]').count() > 0

            # ãƒ¡ãƒ‡ã‚£ã‚¢URL
            media_urls = []
            media_elements = await element.locator('img[src*="pbs.twimg.com/media"]').all()
            for media in media_elements:
                src = await media.get_attribute("src")
                if src:
                    media_urls.append(src)

            return XPost(
                post_id=post_id,
                author_username=author_username,
                author_name=author_name,
                content=content,
                timestamp=timestamp,
                likes=likes,
                retweets=retweets,
                replies=replies,
                views=views,
                is_retweet=is_retweet,
                is_reply=is_reply,
                media_urls=media_urls,
                post_url=f"https://x.com/{author_username}/status/{post_id}"
            )

        except Exception as e:
            logger.debug(f"Parse error: {e}")
            return None

    async def _extract_engagement(self, element: Locator, action: str) -> int:
        """ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ•°ã‚’æŠ½å‡º"""
        try:
            btn = element.locator(f'[data-testid="{action}"]').first
            if await btn.count() > 0:
                text = await btn.inner_text()
                return self._parse_number(text)
        except:
            pass
        return 0

    def _parse_number(self, text: str) -> int:
        """æ•°å€¤æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆ1.5K â†’ 1500ï¼‰"""
        if not text:
            return 0

        text = text.strip().upper()
        multiplier = 1

        if "K" in text:
            multiplier = 1000
            text = text.replace("K", "")
        elif "M" in text:
            multiplier = 1000000
            text = text.replace("M", "")
        elif "ä¸‡" in text:
            multiplier = 10000
            text = text.replace("ä¸‡", "")

        try:
            return int(float(text) * multiplier)
        except:
            return 0

    def _should_include_post(self, post: XPost) -> bool:
        """ãƒã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        # ãƒªãƒ—ãƒ©ã‚¤ãƒ•ã‚£ãƒ«ã‚¿
        if post.is_reply and not self.config.include_replies:
            return False

        # ãƒªãƒ„ã‚¤ãƒ¼ãƒˆãƒ•ã‚£ãƒ«ã‚¿
        if post.is_retweet and not self.config.include_retweets:
            return False

        # ã„ã„ã­æ•°ãƒ•ã‚£ãƒ«ã‚¿
        if post.likes < self.config.min_likes:
            return False

        # RTæ•°ãƒ•ã‚£ãƒ«ã‚¿
        if post.retweets < self.config.min_retweets:
            return False

        return True

    async def _check_account_not_found(self) -> bool:
        """ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ãƒã‚§ãƒƒã‚¯"""
        # 404ãƒšãƒ¼ã‚¸ã¾ãŸã¯"doesn't exist"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        not_found = await self.page.locator('text="doesn\'t exist"').count() > 0
        suspended = await self.page.locator('text="Account suspended"').count() > 0
        return not_found or suspended

    async def _scroll_down(self):
        """ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«"""
        await self.page.evaluate("window.scrollBy(0, window.innerHeight)")
        await asyncio.sleep(self.config.scroll_delay_ms / 1000)

    # ==========================================
    # è¤‡æ•°ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæŠ½å‡º
    # ==========================================

    async def extract_from_accounts(
        self,
        usernames: List[str],
        max_posts_per_account: Optional[int] = None
    ) -> Dict[str, List[XPost]]:
        """è¤‡æ•°ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒã‚¹ãƒˆã‚’æŠ½å‡º"""
        results = {}

        for username in usernames:
            posts = await self.extract_from_account(username, max_posts_per_account)
            results[username] = posts
            await asyncio.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

        return results

    # ==========================================
    # å‡ºåŠ›
    # ==========================================

    def save_json(
        self,
        posts: List[XPost],
        filename: str,
        output_dir: Optional[Path] = None
    ) -> Path:
        """JSONå½¢å¼ã§ä¿å­˜"""
        output_dir = output_dir or DATA_DIR
        filepath = output_dir / f"{filename}.json"

        data = [asdict(p) for p in posts]
        filepath.write_text(json.dumps(data, ensure_ascii=False, indent=2))

        logger.info(f"ğŸ’¾ Saved {len(posts)} posts to {filepath}")
        return filepath

    def save_csv(
        self,
        posts: List[XPost],
        filename: str,
        output_dir: Optional[Path] = None
    ) -> Path:
        """CSVå½¢å¼ã§ä¿å­˜"""
        output_dir = output_dir or DATA_DIR
        filepath = output_dir / f"{filename}.csv"

        if not posts:
            return filepath

        fieldnames = list(asdict(posts[0]).keys())

        with open(filepath, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for post in posts:
                row = asdict(post)
                row["media_urls"] = ";".join(row["media_urls"])  # ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«
                writer.writerow(row)

        logger.info(f"ğŸ’¾ Saved {len(posts)} posts to {filepath}")
        return filepath

    def save_markdown(
        self,
        posts: List[XPost],
        filename: str,
        output_dir: Optional[Path] = None
    ) -> Path:
        """Markdownå½¢å¼ã§ä¿å­˜"""
        output_dir = output_dir or DATA_DIR
        filepath = output_dir / f"{filename}.md"

        lines = [
            f"# X Posts - {filename}",
            f"",
            f"Generated: {datetime.now().isoformat()}",
            f"Total: {len(posts)} posts",
            f"",
            "---",
            ""
        ]

        for post in posts:
            lines.extend([
                f"## @{post.author_username}",
                f"",
                f"> {post.content}",
                f"",
                f"- ğŸ“… {post.timestamp}",
                f"- â¤ï¸ {post.likes} | ğŸ”„ {post.retweets} | ğŸ’¬ {post.replies}",
                f"- ğŸ”— [{post.post_url}]({post.post_url})",
                f"",
                "---",
                ""
            ])

        filepath.write_text("\n".join(lines), encoding="utf-8")
        logger.info(f"ğŸ’¾ Saved {len(posts)} posts to {filepath}")
        return filepath

    def save(
        self,
        posts: List[XPost],
        filename: str,
        format: str = "json",
        output_dir: Optional[Path] = None
    ) -> Path:
        """æŒ‡å®šå½¢å¼ã§ä¿å­˜"""
        if format == "json":
            return self.save_json(posts, filename, output_dir)
        elif format == "csv":
            return self.save_csv(posts, filename, output_dir)
        elif format == "markdown" or format == "md":
            return self.save_markdown(posts, filename, output_dir)
        else:
            raise ValueError(f"Unsupported format: {format}")


# ==========================================
# CLI
# ==========================================

async def main():
    import argparse

    parser = argparse.ArgumentParser(description="X Post Extractor")
    parser.add_argument("usernames", nargs="+", help="æŠ½å‡ºã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼åï¼ˆ@ãªã—ï¼‰")
    parser.add_argument("--max", type=int, default=50, help="ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã”ã¨ã®æœ€å¤§å–å¾—æ•°")
    parser.add_argument("--format", choices=["json", "csv", "markdown"], default="json",
                       help="å‡ºåŠ›å½¢å¼")
    parser.add_argument("--output", help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰")
    parser.add_argument("--session", default="default", help="ã‚»ãƒƒã‚·ãƒ§ãƒ³å")
    parser.add_argument("--min-likes", type=int, default=0, help="æœ€å°ã„ã„ã­æ•°")
    parser.add_argument("--include-replies", action="store_true", help="ãƒªãƒ—ãƒ©ã‚¤ã‚’å«ã‚ã‚‹")
    parser.add_argument("--include-retweets", action="store_true", help="ãƒªãƒ„ã‚¤ãƒ¼ãƒˆã‚’å«ã‚ã‚‹")

    args = parser.parse_args()

    # è¨­å®š
    browser_config = BrowserConfig(session_name=args.session)
    extractor_config = ExtractorConfig(
        max_posts_per_account=args.max,
        min_likes=args.min_likes,
        include_replies=args.include_replies,
        include_retweets=args.include_retweets,
        output_format=args.format
    )

    print("=" * 60)
    print("ğŸ“¥ X Post Extractor")
    print("=" * 60)
    print(f"  Targets: {', '.join(['@' + u for u in args.usernames])}")
    print(f"  Max posts per account: {args.max}")
    print(f"  Output format: {args.format}")
    print("=" * 60)

    async with SessionManager(browser_config) as session:
        # ãƒ­ã‚°ã‚¤ãƒ³ç¢ºèª
        if not await session.ensure_logged_in():
            print("âŒ ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ã§ã™")
            return

        extractor = PostExtractor(session, extractor_config)

        # æŠ½å‡º
        all_posts = []
        for username in args.usernames:
            posts = await extractor.extract_from_account(username)
            all_posts.extend(posts)

        # ä¿å­˜
        if all_posts:
            output_name = args.output or f"x_posts_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            filepath = extractor.save(all_posts, output_name, args.format)
            print(f"\nâœ… å®Œäº†! {len(all_posts)}ä»¶ã®ãƒã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            print(f"   ğŸ“ {filepath}")
        else:
            print("\nâŒ ãƒã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == "__main__":
    asyncio.run(main())
