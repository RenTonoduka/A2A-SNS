"""
Channel Manager
ä¿å­˜æ¸ˆã¿ãƒãƒ£ãƒ³ãƒãƒ«ã®ç®¡ç† + APIã‚­ãƒ£ãƒƒã‚·ãƒ¥

- CSVã§ãƒãƒ£ãƒ³ãƒãƒ«ãƒªã‚¹ãƒˆç®¡ç†
- APIã¯å¿…è¦ãªæ™‚ã ã‘å©ãï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé–“è¨­å®šï¼‰
- å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚‚CSVã«ä¿å­˜
- Claude CodeãŒåˆ†æãƒ»è©•ä¾¡
"""

import os
import sys
import csv
import json
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict, field

# ãƒ‘ã‚¹è¨­å®š
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SNS_DIR = os.path.dirname(os.path.dirname(SCRIPT_DIR))
sys.path.insert(0, SNS_DIR)

DATA_DIR = os.path.join(SCRIPT_DIR, "data")

from _shared.claude_runner import ClaudeRunner


@dataclass
class Channel:
    """ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±"""
    channel_id: str
    channel_name: str
    subscriber_count: int
    video_count: int
    saved_date: str
    last_fetched: str = ""
    notes: str = ""
    total_views: int = 0


@dataclass
class Video:
    """å‹•ç”»æƒ…å ±"""
    video_id: str
    channel_id: str
    channel_name: str
    title: str
    view_count: int
    like_count: int
    comment_count: int
    published_at: str
    fetched_at: str
    subscriber_count_at_fetch: int = 0
    performance_ratio: float = 0.0  # å†ç”Ÿæ•°/ç™»éŒ²è€…æ•°
    duration: str = ""  # ISO 8601å½¢å¼ (PT1M30S = 1åˆ†30ç§’)

    def get_duration_seconds(self) -> int:
        """å‹•ç”»ã®é•·ã•ã‚’ç§’æ•°ã§å–å¾—"""
        if not self.duration:
            return 0
        # ISO 8601å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹ (PT1H2M30S â†’ 3750ç§’)
        import re
        match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', self.duration)
        if not match:
            return 0
        hours = int(match.group(1) or 0)
        minutes = int(match.group(2) or 0)
        seconds = int(match.group(3) or 0)
        return hours * 3600 + minutes * 60 + seconds

    def is_short(self) -> bool:
        """ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ã‹ã©ã†ã‹ï¼ˆ60ç§’ä»¥ä¸‹ï¼‰"""
        return 0 < self.get_duration_seconds() <= 60


class ChannelManager:
    """
    ãƒãƒ£ãƒ³ãƒãƒ«ç®¡ç†ã‚¯ãƒ©ã‚¹

    - ãƒãƒ£ãƒ³ãƒãƒ«ãƒªã‚¹ãƒˆCSVç®¡ç†
    - å‹•ç”»ãƒ‡ãƒ¼ã‚¿CSVç®¡ç†
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¶å¾¡ï¼ˆæ›´æ–°é–“éš”è¨­å®šï¼‰
    """

    def __init__(
        self,
        data_dir: str = DATA_DIR,
        cache_days: int = 7,  # 7æ—¥ã”ã¨ã«æ›´æ–°
        api_key: Optional[str] = None
    ):
        self.data_dir = data_dir
        self.cache_days = cache_days
        self.api_key = api_key or os.environ.get("YOUTUBE_API_KEY")

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.channels_file = os.path.join(data_dir, "channels.csv")
        self.videos_file = os.path.join(data_dir, "videos.csv")
        self.fetch_log_file = os.path.join(data_dir, "fetch_log.csv")

        os.makedirs(data_dir, exist_ok=True)

        # APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰
        self._api = None

        # Claude Runner
        self.claude = ClaudeRunner(timeout=180)

    @property
    def api(self):
        """APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®é…å»¶åˆæœŸåŒ–"""
        if self._api is None:
            if not self.api_key:
                raise ValueError("YouTube API key required")
            from youtube_api import YouTubeAPIClient
            self._api = YouTubeAPIClient(self.api_key)
        return self._api

    # ==========================================
    # ãƒãƒ£ãƒ³ãƒãƒ«ç®¡ç†
    # ==========================================

    def load_channels(self) -> List[Channel]:
        """ãƒãƒ£ãƒ³ãƒãƒ«ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿"""
        if not os.path.exists(self.channels_file):
            return []

        channels = []
        with open(self.channels_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                channels.append(Channel(
                    channel_id=row.get("channel_id", ""),
                    channel_name=row.get("channel_name", ""),
                    subscriber_count=int(row.get("subscriber_count", 0)),
                    video_count=int(row.get("video_count", 0)),
                    saved_date=row.get("saved_date", ""),
                    last_fetched=row.get("last_fetched", ""),
                    notes=row.get("notes", ""),
                    total_views=int(row.get("total_views", 0))
                ))
        return channels

    def save_channels(self, channels: List[Channel]):
        """ãƒãƒ£ãƒ³ãƒãƒ«ãƒªã‚¹ãƒˆä¿å­˜"""
        with open(self.channels_file, "w", encoding="utf-8", newline="") as f:
            fieldnames = ["channel_id", "channel_name", "subscriber_count", "video_count",
                         "saved_date", "last_fetched", "total_views", "notes"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for ch in channels:
                writer.writerow(asdict(ch))

    def add_channel(self, channel_name: str, notes: str = "") -> Optional[Channel]:
        """ãƒãƒ£ãƒ³ãƒãƒ«ã‚’è¿½åŠ ï¼ˆåå‰ã§æ¤œç´¢ã—ã¦IDã‚’å–å¾—ï¼‰"""
        print(f"ğŸ” Searching for: {channel_name}")

        results = self.api.search_channels(channel_name, max_results=1)
        if not results:
            print(f"âŒ Channel not found: {channel_name}")
            return None

        ch_data = results[0]
        channel = Channel(
            channel_id=ch_data.channel_id,
            channel_name=ch_data.title,
            subscriber_count=ch_data.subscriber_count,
            video_count=ch_data.video_count,
            saved_date=datetime.now().strftime("%Y-%m-%d"),
            last_fetched=datetime.now().strftime("%Y-%m-%d"),
            total_views=ch_data.view_count,
            notes=notes
        )

        # æ—¢å­˜ãƒªã‚¹ãƒˆã«è¿½åŠ 
        channels = self.load_channels()
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if any(c.channel_id == channel.channel_id for c in channels):
            print(f"âš ï¸ Channel already exists: {channel_name}")
            return None

        channels.append(channel)
        self.save_channels(channels)

        self._log_fetch("add_channel", channel.channel_id, channel.channel_name)

        print(f"âœ… Added: {channel.channel_name} ({channel.subscriber_count:,} subs)")
        return channel

    def resolve_channel_ids(self) -> int:
        """
        channel_idãŒç©ºã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¤œç´¢ã—ã¦IDã‚’å–å¾—
        """
        channels = self.load_channels()
        resolved = 0

        for ch in channels:
            if not ch.channel_id:
                print(f"ğŸ” Resolving: {ch.channel_name}")
                results = self.api.search_channels(ch.channel_name, max_results=1)

                if results:
                    ch.channel_id = results[0].channel_id
                    ch.subscriber_count = results[0].subscriber_count
                    ch.video_count = results[0].video_count
                    ch.total_views = results[0].view_count
                    ch.last_fetched = datetime.now().strftime("%Y-%m-%d")
                    resolved += 1
                    print(f"  âœ… Found: {ch.channel_id}")
                    self._log_fetch("resolve_id", ch.channel_id, ch.channel_name)
                else:
                    print(f"  âŒ Not found")

        self.save_channels(channels)
        return resolved

    def needs_update(self, channel: Channel) -> bool:
        """æ›´æ–°ãŒå¿…è¦ã‹åˆ¤å®š"""
        if not channel.last_fetched:
            return True

        try:
            last = datetime.strptime(channel.last_fetched, "%Y-%m-%d")
            return datetime.now() - last > timedelta(days=self.cache_days)
        except:
            return True

    def get_channels_needing_update(self) -> List[Channel]:
        """æ›´æ–°ãŒå¿…è¦ãªãƒãƒ£ãƒ³ãƒãƒ«ä¸€è¦§"""
        return [ch for ch in self.load_channels() if self.needs_update(ch)]

    # ==========================================
    # å‹•ç”»ãƒ‡ãƒ¼ã‚¿ç®¡ç†
    # ==========================================

    def load_videos(self, channel_id: Optional[str] = None, exclude_shorts: bool = False) -> List[Video]:
        """å‹•ç”»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿

        Args:
            channel_id: ç‰¹å®šãƒãƒ£ãƒ³ãƒãƒ«ã®ã¿å–å¾—
            exclude_shorts: ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ï¼ˆ60ç§’ä»¥ä¸‹ï¼‰ã‚’é™¤å¤–
        """
        if not os.path.exists(self.videos_file):
            return []

        videos = []
        with open(self.videos_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if channel_id and row.get("channel_id") != channel_id:
                    continue
                video = Video(
                    video_id=row.get("video_id", ""),
                    channel_id=row.get("channel_id", ""),
                    channel_name=row.get("channel_name", ""),
                    title=row.get("title", ""),
                    view_count=int(row.get("view_count", 0)),
                    like_count=int(row.get("like_count", 0)),
                    comment_count=int(row.get("comment_count", 0)),
                    published_at=row.get("published_at", ""),
                    fetched_at=row.get("fetched_at", ""),
                    subscriber_count_at_fetch=int(row.get("subscriber_count_at_fetch", 0)),
                    performance_ratio=float(row.get("performance_ratio", 0)),
                    duration=row.get("duration", "")
                )
                # ã‚·ãƒ§ãƒ¼ãƒˆé™¤å¤–
                if exclude_shorts and video.is_short():
                    continue
                videos.append(video)
        return videos

    def save_videos(self, videos: List[Video], append: bool = False):
        """å‹•ç”»ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        mode = "a" if append else "w"
        file_exists = os.path.exists(self.videos_file) and append

        with open(self.videos_file, mode, encoding="utf-8", newline="") as f:
            fieldnames = ["video_id", "channel_id", "channel_name", "title",
                         "view_count", "like_count", "comment_count",
                         "published_at", "fetched_at", "subscriber_count_at_fetch",
                         "performance_ratio", "duration"]
            writer = csv.DictWriter(f, fieldnames=fieldnames)

            if not file_exists:
                writer.writeheader()

            for v in videos:
                data = asdict(v)
                # ãƒ¡ã‚½ãƒƒãƒ‰ã¯é™¤å¤–ï¼ˆasdictã«ã¯å«ã¾ã‚Œãªã„ãŒå¿µã®ãŸã‚ï¼‰
                data.pop('get_duration_seconds', None)
                data.pop('is_short', None)
                writer.writerow(data)

    def fetch_channel_videos(
        self,
        channel: Channel,
        top_n: int = 10,
        force: bool = False
    ) -> List[Video]:
        """
        ãƒãƒ£ãƒ³ãƒãƒ«ã®ä¸Šä½å‹•ç”»ã‚’å–å¾—

        Args:
            channel: å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«
            top_n: å–å¾—ã™ã‚‹å‹•ç”»æ•°
            force: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶å–å¾—
        """
        if not channel.channel_id:
            print(f"âš ï¸ No channel_id for: {channel.channel_name}")
            return []

        if not force and not self.needs_update(channel):
            print(f"ğŸ“¦ Using cached data for: {channel.channel_name}")
            return self.load_videos(channel.channel_id)

        print(f"ğŸ“¡ Fetching videos for: {channel.channel_name}")

        raw_videos = self.api.get_channel_videos(
            channel.channel_id,
            max_results=top_n,
            order="viewCount"
        )

        if not raw_videos:
            return []

        videos = []
        fetched_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        for v in raw_videos:
            ratio = v.view_count / channel.subscriber_count if channel.subscriber_count > 0 else 0
            videos.append(Video(
                video_id=v.video_id,
                channel_id=channel.channel_id,
                channel_name=channel.channel_name,
                title=v.title,
                view_count=v.view_count,
                like_count=v.like_count,
                comment_count=v.comment_count,
                published_at=v.published_at,
                fetched_at=fetched_at,
                subscriber_count_at_fetch=channel.subscriber_count,
                performance_ratio=round(ratio, 2),
                duration=v.duration  # ISO 8601å½¢å¼ (PT1M30S)
            ))

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©²å½“ãƒãƒ£ãƒ³ãƒãƒ«ä»¥å¤–ã‚’æ®‹ã™
        existing = [v for v in self.load_videos() if v.channel_id != channel.channel_id]
        self.save_videos(existing + videos)

        # ãƒãƒ£ãƒ³ãƒãƒ«ã®æœ€çµ‚å–å¾—æ—¥ã‚’æ›´æ–°
        channels = self.load_channels()
        for ch in channels:
            if ch.channel_id == channel.channel_id:
                ch.last_fetched = datetime.now().strftime("%Y-%m-%d")
                break
        self.save_channels(channels)

        self._log_fetch("fetch_videos", channel.channel_id, channel.channel_name, len(videos))

        return videos

    def fetch_all_channels(self, top_n: int = 10, force: bool = False) -> Dict[str, List[Video]]:
        """å…¨ãƒãƒ£ãƒ³ãƒãƒ«ã®å‹•ç”»ã‚’å–å¾—"""
        channels = self.load_channels()
        result = {}

        for ch in channels:
            if not ch.channel_id:
                continue
            videos = self.fetch_channel_videos(ch, top_n, force)
            if videos:
                result[ch.channel_name] = videos

        return result

    # ==========================================
    # å„ªç§€å‹•ç”»ã®ç™ºè¦‹
    # ==========================================

    def find_outstanding_videos(
        self,
        threshold: float = 2.0,
        min_views: int = 1000,
        exclude_shorts: bool = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚·ãƒ§ãƒ¼ãƒˆé™¤å¤–
    ) -> List[Video]:
        """
        å„ªç§€ãªå‹•ç”»ã‚’ç™ºè¦‹ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§é™¤å¤–ï¼‰

        Args:
            threshold: PRã—ãã„å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2.0 = ç™»éŒ²è€…ã®2å€ä»¥ä¸Šï¼‰
            min_views: æœ€å°å†ç”Ÿæ•°
            exclude_shorts: ã‚·ãƒ§ãƒ¼ãƒˆå‹•ç”»ï¼ˆ60ç§’ä»¥ä¸‹ï¼‰ã‚’é™¤å¤–

        åŸºæº–: ç™»éŒ²è€…æ•°ã«å¯¾ã—ã¦å†ç”Ÿæ•°ãŒç•°å¸¸ã«é«˜ã„
        threshold=2.0 â†’ ç™»éŒ²è€…ã®2å€ä»¥ä¸Šã®å†ç”Ÿæ•°
        """
        all_videos = self.load_videos(exclude_shorts=exclude_shorts)

        outstanding = [
            v for v in all_videos
            if v.performance_ratio >= threshold and v.view_count >= min_views
        ]

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”ã§é™é †ã‚½ãƒ¼ãƒˆ
        outstanding.sort(key=lambda x: x.performance_ratio, reverse=True)

        return outstanding

    def analyze_outstanding_videos(self, threshold: float = 2.0) -> Dict[str, Any]:
        """
        å„ªç§€å‹•ç”»ã‚’Claude Codeã«åˆ†æã•ã›ã‚‹

        ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        Claude Code: åˆ†æãƒ»è©•ä¾¡ãƒ»åˆ¤æ–­
        """
        outstanding = self.find_outstanding_videos(threshold)

        if not outstanding:
            return {"error": "No outstanding videos found"}

        # ãƒ‡ãƒ¼ã‚¿æ•´å½¢
        data = {
            "threshold": threshold,
            "total_videos": len(self.load_videos()),
            "outstanding_count": len(outstanding),
            "videos": [
                {
                    "title": v.title,
                    "channel": v.channel_name,
                    "views": v.view_count,
                    "subscribers": v.subscriber_count_at_fetch,
                    "ratio": v.performance_ratio,
                    "likes": v.like_count,
                    "comments": v.comment_count,
                    "published": v.published_at[:10] if v.published_at else ""
                }
                for v in outstanding[:30]  # ä¸Šä½30ä»¶
            ]
        }

        # Claude Codeã«åˆ†æã‚’ä¾é ¼
        prompt = f"""ã‚ãªãŸã¯YouTubeãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã¯ã€Œç™»éŒ²è€…æ•°ã«å¯¾ã—ã¦å†ç”Ÿæ•°ãŒç•°å¸¸ã«é«˜ã„ã€å„ªç§€ãªå‹•ç”»ã®ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚
ï¼ˆperformance_ratio = å†ç”Ÿæ•° / ç™»éŒ²è€…æ•°ï¼‰

## ãƒ‡ãƒ¼ã‚¿
```json
{json.dumps(data, ensure_ascii=False, indent=2)}
```

## åˆ†æã‚¿ã‚¹ã‚¯

### 1. ã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
å„ªç§€å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«ã«å…±é€šã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š
- æ•°å­—ã®ä½¿ã„æ–¹
- ãƒ•ãƒƒã‚¯ãƒ¯ãƒ¼ãƒ‰ï¼ˆã€Œè¡æ’ƒã€ã€Œå®Œå…¨ã€ã€Œæœ€æ–°ã€ç­‰ï¼‰
- æ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã€ã€‘ã€ï½œã®ä½¿ã„æ–¹ï¼‰

### 2. æˆåŠŸè¦å› åˆ†æ
ãªãœã“ã‚Œã‚‰ã®å‹•ç”»ã¯ç™»éŒ²è€…æ•°ä»¥ä¸Šã®ãƒªãƒ¼ãƒã‚’ç²å¾—ã§ããŸã®ã‹ï¼Ÿ
- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¦å› 
- ã‚¿ã‚¤ãƒŸãƒ³ã‚°è¦å› 
- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¦å› ï¼ˆæ¨æ¸¬ï¼‰

### 3. å†ç¾å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
ã“ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å°ãå‡ºã›ã‚‹ã€å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¤ãƒˆãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’5ã¤æç¤ºã—ã¦ãã ã•ã„ã€‚

### 4. ç‹™ã„ç›®ãƒ†ãƒ¼ãƒ
ã©ã®ãƒãƒ£ãƒ³ãƒãƒ«ã®ã©ã®ãƒ†ãƒ¼ãƒãŒç‰¹ã«æˆåŠŸã—ã¦ã„ã‚‹ã‹ï¼Ÿ

## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
{{
    "title_patterns": {{
        "number_usage": ["ãƒ‘ã‚¿ãƒ¼ãƒ³1", "ãƒ‘ã‚¿ãƒ¼ãƒ³2"],
        "hook_words": ["ãƒ•ãƒƒã‚¯1", "ãƒ•ãƒƒã‚¯2"],
        "structure_patterns": ["æ§‹é€ 1", "æ§‹é€ 2"]
    }},
    "success_factors": ["è¦å› 1", "è¦å› 2", "è¦å› 3"],
    "title_templates": [
        "ã€æ•°å­—ã€‘ã€‡ã€‡ãŒâ–³â–³ã™ã‚‹æ–¹æ³•",
        "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ2",
        ...
    ],
    "hot_themes": [
        {{"theme": "ãƒ†ãƒ¼ãƒ", "channel": "ãƒãƒ£ãƒ³ãƒãƒ«", "reason": "ç†ç”±"}}
    ],
    "actionable_insights": ["ã‚¤ãƒ³ã‚µã‚¤ãƒˆ1", "ã‚¤ãƒ³ã‚µã‚¤ãƒˆ2"],
    "summary": "2-3æ–‡ã®ç·æ‹¬"
}}

JSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

        result = self.claude.run(prompt)

        if not result["success"]:
            return {"error": result["error"], "data": data}

        try:
            analysis = json.loads(self._extract_json(result["output"]))
            return {
                "outstanding_videos": data["videos"],
                "analysis": analysis,
                "analyzed_at": datetime.now().isoformat()
            }
        except json.JSONDecodeError:
            return {"raw_analysis": result["output"], "data": data}

    # ==========================================
    # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    # ==========================================

    def _log_fetch(self, action: str, channel_id: str, channel_name: str, count: int = 0):
        """APIå‘¼ã³å‡ºã—ãƒ­ã‚°"""
        file_exists = os.path.exists(self.fetch_log_file)

        with open(self.fetch_log_file, "a", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(["timestamp", "action", "channel_id", "channel_name", "count"])
            writer.writerow([
                datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                action,
                channel_id,
                channel_name,
                count
            ])

    def _extract_json(self, text: str) -> str:
        """JSONã‚’æŠ½å‡º"""
        if "```json" in text:
            start = text.find("```json") + 7
            end = text.find("```", start)
            if end > start:
                return text[start:end].strip()

        if "```" in text:
            start = text.find("```") + 3
            end = text.find("```", start)
            if end > start:
                return text[start:end].strip()

        start = text.find("{")
        if start >= 0:
            depth = 0
            for i, c in enumerate(text[start:]):
                if c == "{":
                    depth += 1
                elif c == "}":
                    depth -= 1
                    if depth == 0:
                        return text[start:start + i + 1]
        return text

    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±"""
        channels = self.load_channels()
        videos = self.load_videos()

        return {
            "total_channels": len(channels),
            "channels_with_id": len([c for c in channels if c.channel_id]),
            "channels_needing_update": len(self.get_channels_needing_update()),
            "total_videos": len(videos),
            "outstanding_videos_2x": len(self.find_outstanding_videos(2.0)),
            "outstanding_videos_5x": len(self.find_outstanding_videos(5.0)),
            "cache_days": self.cache_days
        }

    # ==========================================
    # è‡ªå‹•æ¢ç´¢ï¼ˆãƒã‚ºå‹•ç”»ãŒãªã‘ã‚Œã°é–¢é€£ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¢ç´¢ï¼‰
    # ==========================================

    def auto_discover_buzz(
        self,
        threshold: float = 5.0,
        min_views: int = 10000,
        days: int = 90,
        max_new_channels: int = 10
    ) -> Dict[str, Any]:
        """
        ãƒã‚ºå‹•ç”»ã‚’è‡ªå‹•æ¢ç´¢

        1. ç™»éŒ²æ¸ˆã¿ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰ãƒã‚ºå‹•ç”»ã‚’æ¤œç´¢
        2. è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€é–¢é€£ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¢ç´¢
        3. ç›´è¿‘Næ—¥ä»¥å†…ã®ãƒã‚ºå‹•ç”»ã®ã¿å¯¾è±¡

        Args:
            threshold: PRã—ãã„å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5.0ï¼‰
            min_views: æœ€å°å†ç”Ÿæ•°
            days: ç›´è¿‘Næ—¥ä»¥å†…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ90æ—¥=3ãƒ¶æœˆï¼‰
            max_new_channels: æ¢ç´¢ã™ã‚‹æ–°è¦ãƒãƒ£ãƒ³ãƒãƒ«æ•°

        Returns:
            {
                "buzz_videos": [...],
                "new_channels_discovered": [...],
                "source": "registered" | "related"
            }
        """
        from datetime import datetime, timedelta

        result = {
            "buzz_videos": [],
            "new_channels_discovered": [],
            "source": "registered",
            "searched_at": datetime.now().isoformat()
        }

        cutoff_date = datetime.now() - timedelta(days=days)

        # Step 1: ç™»éŒ²æ¸ˆã¿ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰ãƒã‚ºå‹•ç”»ã‚’æ¢ã™
        print(f"ğŸ” Step 1: ç™»éŒ²ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰ç›´è¿‘{days}æ—¥ã®ãƒã‚ºå‹•ç”»ã‚’æ¤œç´¢...")
        outstanding = self.find_outstanding_videos(
            threshold=threshold,
            min_views=min_views,
            exclude_shorts=True
        )

        # ç›´è¿‘Næ—¥ä»¥å†…ã«ãƒ•ã‚£ãƒ«ã‚¿
        recent_buzz = []
        for v in outstanding:
            try:
                pub_date = datetime.fromisoformat(v.published_at.replace('Z', '+00:00'))
                if pub_date.replace(tzinfo=None) >= cutoff_date:
                    recent_buzz.append(v)
            except:
                pass

        if recent_buzz:
            print(f"  âœ… {len(recent_buzz)}ä»¶ã®ãƒã‚ºå‹•ç”»ã‚’ç™ºè¦‹ï¼")
            result["buzz_videos"] = [
                {
                    "video_id": v.video_id,
                    "title": v.title,
                    "channel_name": v.channel_name,
                    "view_count": v.view_count,
                    "performance_ratio": v.performance_ratio,
                    "published_at": v.published_at
                }
                for v in recent_buzz[:20]
            ]
            return result

        # Step 2: ãƒã‚ºå‹•ç”»ãŒãªã‘ã‚Œã°é–¢é€£ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¢ç´¢
        print(f"  âš ï¸ ç™»éŒ²ãƒãƒ£ãƒ³ãƒãƒ«ã«ãƒã‚ºå‹•ç”»ãªã—")
        print(f"ğŸ” Step 2: é–¢é€£ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¢ç´¢...")

        result["source"] = "related"
        channels = self.load_channels()
        discovered_channels = []
        all_buzz = []

        for ch in channels[:5]:  # ä¸Šä½5ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰é–¢é€£ã‚’æ¢ç´¢
            if not ch.channel_id:
                continue

            print(f"  ğŸ“¡ {ch.channel_name} ã®é–¢é€£ãƒãƒ£ãƒ³ãƒãƒ«ã‚’æ¤œç´¢...")
            related = self.api.find_related_channels(ch.channel_id, max_results=5)

            for rel_ch in related:
                # æ—¢ã«ç™»éŒ²æ¸ˆã¿ãªã‚‰é£›ã°ã™
                if any(c.channel_id == rel_ch.channel_id for c in channels):
                    continue
                if any(d["channel_id"] == rel_ch.channel_id for d in discovered_channels):
                    continue

                # ç›´è¿‘Næ—¥ã®å‹•ç”»ã‚’å–å¾—
                recent_videos = self.api.get_recent_videos(
                    rel_ch.channel_id,
                    max_results=20,
                    days=days
                )

                # ãƒã‚ºå‹•ç”»ã‚’æ¤œå‡º
                for v in recent_videos:
                    if rel_ch.subscriber_count > 0:
                        pr = v.view_count / rel_ch.subscriber_count
                        # ã‚·ãƒ§ãƒ¼ãƒˆé™¤å¤–ï¼ˆ60ç§’ä»¥ä¸‹ï¼‰
                        duration_match = None
                        if v.duration:
                            import re
                            duration_match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', v.duration)
                        if duration_match:
                            hours = int(duration_match.group(1) or 0)
                            minutes = int(duration_match.group(2) or 0)
                            seconds = int(duration_match.group(3) or 0)
                            total_seconds = hours * 3600 + minutes * 60 + seconds
                            if 0 < total_seconds <= 60:
                                continue  # ã‚·ãƒ§ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—

                        if pr >= threshold and v.view_count >= min_views:
                            all_buzz.append({
                                "video_id": v.video_id,
                                "title": v.title,
                                "channel_id": rel_ch.channel_id,
                                "channel_name": rel_ch.title,
                                "view_count": v.view_count,
                                "subscriber_count": rel_ch.subscriber_count,
                                "performance_ratio": round(pr, 2),
                                "published_at": v.published_at
                            })

                            # ã“ã®æ–°ãƒãƒ£ãƒ³ãƒãƒ«ã‚’è¨˜éŒ²
                            if not any(d["channel_id"] == rel_ch.channel_id for d in discovered_channels):
                                discovered_channels.append({
                                    "channel_id": rel_ch.channel_id,
                                    "channel_name": rel_ch.title,
                                    "subscriber_count": rel_ch.subscriber_count,
                                    "video_count": rel_ch.video_count
                                })

                if len(discovered_channels) >= max_new_channels:
                    break
            if len(discovered_channels) >= max_new_channels:
                break

        # PRé †ã«ã‚½ãƒ¼ãƒˆ
        all_buzz.sort(key=lambda x: x["performance_ratio"], reverse=True)

        result["buzz_videos"] = all_buzz[:20]
        result["new_channels_discovered"] = discovered_channels

        if all_buzz:
            print(f"  âœ… é–¢é€£ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰{len(all_buzz)}ä»¶ã®ãƒã‚ºå‹•ç”»ã‚’ç™ºè¦‹ï¼")
            print(f"  ğŸ“Œ æ–°è¦ãƒãƒ£ãƒ³ãƒãƒ«{len(discovered_channels)}ä»¶ã‚’ç™ºè¦‹")
        else:
            print(f"  âŒ ãƒã‚ºå‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        return result

    def add_discovered_channels(self, discovered: List[Dict]) -> int:
        """
        ç™ºè¦‹ã—ãŸãƒãƒ£ãƒ³ãƒãƒ«ã‚’channels.csvã«è¿½åŠ 

        Args:
            discovered: auto_discover_buzz()ã§è¿”ã•ã‚ŒãŸnew_channels_discovered

        Returns:
            è¿½åŠ ã—ãŸä»¶æ•°
        """
        channels = self.load_channels()
        added = 0

        for d in discovered:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if any(c.channel_id == d["channel_id"] for c in channels):
                continue

            channels.append(Channel(
                channel_id=d["channel_id"],
                channel_name=d["channel_name"],
                subscriber_count=d["subscriber_count"],
                video_count=d["video_count"],
                saved_date=datetime.now().strftime("%Y-%m-%d"),
                last_fetched=datetime.now().strftime("%Y-%m-%d"),
                notes="auto-discovered"
            ))
            added += 1
            print(f"  âœ… è¿½åŠ : {d['channel_name']} ({d['subscriber_count']:,} subs)")

        if added > 0:
            self.save_channels(channels)

        return added

    def print_status(self):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º"""
        stats = self.get_stats()
        print("\nğŸ“Š Channel Manager Status")
        print("=" * 50)
        print(f"Total Channels: {stats['total_channels']}")
        print(f"  - With ID: {stats['channels_with_id']}")
        print(f"  - Needs Update: {stats['channels_needing_update']}")
        print(f"Total Videos: {stats['total_videos']}")
        print(f"Outstanding Videos:")
        print(f"  - 2x+ ratio: {stats['outstanding_videos_2x']}")
        print(f"  - 5x+ ratio: {stats['outstanding_videos_5x']}")
        print(f"Cache Period: {stats['cache_days']} days")
        print("=" * 50)


# CLI
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="YouTube Channel Manager")
    parser.add_argument("command", choices=["status", "resolve", "fetch", "outstanding", "analyze", "add", "discover"])
    parser.add_argument("--channel", "-c", help="Channel name (for add command)")
    parser.add_argument("--force", "-f", action="store_true", help="Force fetch (ignore cache)")
    parser.add_argument("--threshold", "-t", type=float, default=2.0, help="Outstanding threshold")
    parser.add_argument("--top", "-n", type=int, default=10, help="Videos per channel")
    args = parser.parse_args()

    manager = ChannelManager()

    if args.command == "status":
        manager.print_status()

    elif args.command == "resolve":
        print("ğŸ” Resolving channel IDs...")
        count = manager.resolve_channel_ids()
        print(f"âœ… Resolved {count} channels")

    elif args.command == "fetch":
        print(f"ğŸ“¡ Fetching videos (top {args.top} per channel)...")
        result = manager.fetch_all_channels(top_n=args.top, force=args.force)
        print(f"âœ… Fetched from {len(result)} channels")

    elif args.command == "outstanding":
        print(f"â­ Finding outstanding videos (threshold: {args.threshold}x)...")
        videos = manager.find_outstanding_videos(args.threshold)
        print(f"\nFound {len(videos)} outstanding videos:\n")
        for i, v in enumerate(videos[:20], 1):
            print(f"{i:2}. {v.title[:50]}...")
            print(f"    Channel: {v.channel_name}")
            print(f"    Views: {v.view_count:,} | Subs: {v.subscriber_count_at_fetch:,} | Ratio: {v.performance_ratio}x")
            print()

    elif args.command == "analyze":
        print(f"ğŸ”¬ Analyzing outstanding videos (threshold: {args.threshold}x)...")
        result = manager.analyze_outstanding_videos(args.threshold)

        if "error" in result:
            print(f"âŒ Error: {result['error']}")
        elif "analysis" in result:
            analysis = result["analysis"]
            print("\nğŸ’¡ Analysis Results")
            print("-" * 50)

            if "title_templates" in analysis:
                print("\nTitle Templates:")
                for t in analysis["title_templates"][:5]:
                    print(f"  â€¢ {t}")

            if "success_factors" in analysis:
                print("\nSuccess Factors:")
                for f in analysis["success_factors"][:5]:
                    print(f"  â€¢ {f}")

            if "actionable_insights" in analysis:
                print("\nActionable Insights:")
                for a in analysis["actionable_insights"][:5]:
                    print(f"  â€¢ {a}")

            if "summary" in analysis:
                print(f"\nSummary: {analysis['summary']}")

            # çµæœã‚’ä¿å­˜
            output_file = os.path.join(DATA_DIR, f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… Saved to: {output_file}")

    elif args.command == "add":
        if not args.channel:
            print("âŒ Please specify channel name with --channel")
        else:
            manager.add_channel(args.channel)

    elif args.command == "discover":
        print(f"ğŸš€ Auto-discovering buzz videos (threshold: {args.threshold}x, last 90 days)...")
        result = manager.auto_discover_buzz(
            threshold=args.threshold,
            min_views=10000,
            days=90
        )

        if result["buzz_videos"]:
            print(f"\nğŸ”¥ Found {len(result['buzz_videos'])} buzz videos:")
            for i, v in enumerate(result["buzz_videos"][:10], 1):
                print(f"\n{i:2}. {v['title'][:50]}...")
                print(f"    Channel: {v['channel_name']}")
                print(f"    Views: {v['view_count']:,} | PR: {v['performance_ratio']}x")

            # æ–°è¦ãƒãƒ£ãƒ³ãƒãƒ«ãŒã‚ã‚Œã°è¿½åŠ ææ¡ˆ
            if result["new_channels_discovered"]:
                print(f"\nğŸ“Œ New channels discovered: {len(result['new_channels_discovered'])}")
                for ch in result["new_channels_discovered"]:
                    print(f"  â€¢ {ch['channel_name']} ({ch['subscriber_count']:,} subs)")

                # è¿½åŠ ã™ã‚‹ã‹ç¢ºèª
                add = input("\nè¿½åŠ ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
                if add == "y":
                    added = manager.add_discovered_channels(result["new_channels_discovered"])
                    print(f"âœ… {added}ä»¶ã®ãƒãƒ£ãƒ³ãƒãƒ«ã‚’è¿½åŠ ã—ã¾ã—ãŸ")

            # ãƒ¡ãƒ¼ãƒ«é€šçŸ¥
            if result["buzz_videos"] and args.threshold >= 5.0:
                try:
                    sys.path.insert(0, os.path.join(SNS_DIR, "_shared"))
                    from google_notifier import notify_buzz_videos
                    notify_buzz_videos(result["buzz_videos"], args.threshold)
                    print("ğŸ“§ ãƒ¡ãƒ¼ãƒ«é€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
                except Exception as e:
                    print(f"âš ï¸ ãƒ¡ãƒ¼ãƒ«é€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")

            # å°æœ¬ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³èµ·å‹•
            print("\n" + "=" * 50)
            print("ğŸ“ å°æœ¬ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’èµ·å‹•ã—ã¾ã™...")
            print("=" * 50)

            try:
                sys.path.insert(0, os.path.join(SNS_DIR, "_shared"))
                from buzz_to_script_pipeline import BuzzToScriptPipeline

                pipeline = BuzzToScriptPipeline()
                pipeline_result = pipeline.process_buzz_videos(result["buzz_videos"])

                print("\nğŸ“‹ ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå–å¾—æŒ‡ç¤º:")
                print("-" * 40)
                print(pipeline_result["transcript_instructions"])

                # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                output_file = os.path.join(DATA_DIR, f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump({
                        "buzz_videos": result["buzz_videos"],
                        "pipeline": pipeline_result,
                        "generated_at": datetime.now().isoformat()
                    }, f, ensure_ascii=False, indent=2)
                print(f"\nâœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’ä¿å­˜: {output_file}")
                print("\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: MCPã§ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å–å¾—å¾Œã€Script Writer Agent (port 8113) ã«é€ä¿¡")

            except Exception as e:
                print(f"âš ï¸ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            print("âŒ ãƒã‚ºå‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
