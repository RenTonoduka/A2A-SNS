"""
YouTube Sheets Logger - ãƒã‚ºå‹•ç”»ãƒ»ãƒãƒ£ãƒ³ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¨˜éŒ²
MCPçµŒç”±ã§Google Sheetsã«æ›¸ãè¾¼ã¿
"""

import json
import os
import csv
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
from pathlib import Path

SCRIPT_DIR = Path(__file__).parent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class YouTubeSheetsLogger:
    """
    YouTube ãƒ‡ãƒ¼ã‚¿ã‚’ Google Sheets ã«è¨˜éŒ²

    ä½¿ç”¨æ–¹æ³•:
    1. MCPçµŒç”±: Claude Code CLIã®MCPãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨
    2. ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥: MCPæœªä½¿ç”¨æ™‚ã¯JSONã«ä¿å­˜
    """

    def __init__(self, spreadsheet_id: Optional[str] = None):
        self.spreadsheet_id = spreadsheet_id or os.getenv("YOUTUBE_SHEETS_ID", "")
        self.cache_dir = SCRIPT_DIR / "data" / "sheets_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ã‚·ãƒ¼ãƒˆå
        self.buzz_sheet = "ãƒã‚ºå‹•ç”»"
        self.videos_sheet = "å‹•ç”»ä¸€è¦§"
        self.channels_sheet = "ãƒãƒ£ãƒ³ãƒãƒ«ä¸€è¦§"

    def _get_cache_file(self, sheet_name: str) -> Path:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        safe_name = sheet_name.replace("/", "_").replace(" ", "_")
        return self.cache_dir / f"{safe_name}.json"

    def _load_cache(self, sheet_name: str) -> List[Dict]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿"""
        cache_file = self._get_cache_file(sheet_name)
        if cache_file.exists():
            try:
                return json.loads(cache_file.read_text())
            except:
                pass
        return []

    def _save_cache(self, sheet_name: str, data: List[Dict]):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        cache_file = self._get_cache_file(sheet_name)
        cache_file.write_text(json.dumps(data, ensure_ascii=False, indent=2))

    # ==========================================
    # ãƒã‚ºå‹•ç”»è¨˜éŒ²
    # ==========================================

    def log_buzz_video(self, video: Dict[str, Any]) -> bool:
        """ãƒã‚ºå‹•ç”»ã‚’è¨˜éŒ²"""
        record = {
            "timestamp": datetime.now().isoformat(),
            "video_id": video.get("video_id", ""),
            "title": video.get("title", "")[:200],
            "channel_name": video.get("channel_name", ""),
            "view_count": video.get("view_count", 0),
            "performance_ratio": round(video.get("performance_ratio", 0), 2),
            "published_at": video.get("published_at", ""),
            "url": f"https://www.youtube.com/watch?v={video.get('video_id', '')}"
        }

        # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        cache = self._load_cache(self.buzz_sheet)

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜video_idã¯è¿½åŠ ã—ãªã„ï¼‰
        existing_ids = {r.get("video_id") for r in cache}
        if record["video_id"] in existing_ids:
            logger.info(f"â­ï¸ Skipped duplicate: {record['title'][:30]}")
            return False

        cache.append(record)
        self._save_cache(self.buzz_sheet, cache)

        logger.info(f"ğŸ“Š Logged buzz video: {record['title'][:40]}")
        return True

    def log_buzz_videos(self, videos: List[Dict[str, Any]]) -> int:
        """è¤‡æ•°ã®ãƒã‚ºå‹•ç”»ã‚’è¨˜éŒ²"""
        count = 0
        for video in videos:
            if self.log_buzz_video(video):
                count += 1
        logger.info(f"ğŸ“Š Total {count} new buzz videos logged")
        return count

    # ==========================================
    # videos.csvå…¨ä½“ã‚’è¨˜éŒ²
    # ==========================================

    def log_all_videos_from_csv(self, csv_path: Optional[str] = None) -> int:
        """videos.csvã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        if csv_path is None:
            csv_path = SCRIPT_DIR / "research" / "data" / "videos.csv"

        csv_path = Path(csv_path)
        if not csv_path.exists():
            logger.error(f"âŒ CSV not found: {csv_path}")
            return 0

        records = []
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                records.append({
                    "video_id": row.get("video_id", ""),
                    "title": row.get("title", "")[:200],
                    "channel_name": row.get("channel_name", ""),
                    "view_count": int(row.get("view_count", 0) or 0),
                    "like_count": int(row.get("like_count", 0) or 0),
                    "comment_count": int(row.get("comment_count", 0) or 0),
                    "avg_view": float(row.get("avg_view", 0) or 0),
                    "performance_ratio": round(float(row.get("performance_ratio", 0) or 0), 2),
                    "published_at": row.get("published_at", ""),
                    "url": f"https://www.youtube.com/watch?v={row.get('video_id', '')}"
                })

        self._save_cache(self.videos_sheet, records)
        logger.info(f"ğŸ“Š Logged {len(records)} videos from CSV")
        return len(records)

    # ==========================================
    # ãƒãƒ£ãƒ³ãƒãƒ«ä¸€è¦§
    # ==========================================

    def log_channels_from_csv(self, csv_path: Optional[str] = None) -> int:
        """channels.csvã®ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        if csv_path is None:
            csv_path = SCRIPT_DIR / "research" / "data" / "channels.csv"

        csv_path = Path(csv_path)
        if not csv_path.exists():
            logger.error(f"âŒ CSV not found: {csv_path}")
            return 0

        records = []
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                records.append({
                    "channel_id": row.get("channel_id", ""),
                    "channel_name": row.get("channel_name", ""),
                    "subscriber_count": int(row.get("subscriber_count", 0) or 0),
                    "video_count": int(row.get("video_count", 0) or 0),
                    "url": f"https://www.youtube.com/channel/{row.get('channel_id', '')}"
                })

        self._save_cache(self.channels_sheet, records)
        logger.info(f"ğŸ“Š Logged {len(records)} channels from CSV")
        return len(records)

    # ==========================================
    # å–å¾—
    # ==========================================

    def get_buzz_videos(self, limit: int = 50) -> List[Dict]:
        """è¨˜éŒ²ã•ã‚ŒãŸãƒã‚ºå‹•ç”»ã‚’å–å¾—"""
        cache = self._load_cache(self.buzz_sheet)
        return cache[-limit:]

    def get_all_videos(self) -> List[Dict]:
        """å…¨å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return self._load_cache(self.videos_sheet)

    # ==========================================
    # MCPçµŒç”±ã§ã®æ›¸ãè¾¼ã¿
    # ==========================================

    def prepare_sheets_update(self, sheet_name: str, data: List[Dict]) -> Dict[str, Any]:
        """
        ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›´æ–°ç”¨ã®MCPãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        """
        if not self.spreadsheet_id:
            return {"error": "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° YOUTUBE_SHEETS_ID ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"}

        if not data:
            return {"error": "æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"}

        headers = list(data[0].keys())
        rows = [[str(row.get(h, "")) for h in headers] for row in data]

        return {
            "spreadsheetId": self.spreadsheet_id,
            "range": f"{sheet_name}!A1",
            "data": [headers] + rows
        }

    def sync_buzz_to_sheets(self) -> Dict[str, Any]:
        """ãƒã‚ºå‹•ç”»ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«åŒæœŸ"""
        data = self._load_cache(self.buzz_sheet)
        return self.prepare_sheets_update(self.buzz_sheet, data)

    def sync_videos_to_sheets(self) -> Dict[str, Any]:
        """å…¨å‹•ç”»ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«åŒæœŸ"""
        data = self._load_cache(self.videos_sheet)
        return self.prepare_sheets_update(self.videos_sheet, data)


# ==========================================
# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
# ==========================================

_logger_instance: Optional[YouTubeSheetsLogger] = None


def get_sheets_logger() -> YouTubeSheetsLogger:
    """ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _logger_instance
    if _logger_instance is None:
        _logger_instance = YouTubeSheetsLogger()
    return _logger_instance


# ==========================================
# CLI
# ==========================================

def main():
    import argparse

    parser = argparse.ArgumentParser(description="YouTube Sheets Logger")
    parser.add_argument("command", choices=["list-buzz", "import-csv", "sync", "export"],
                       help="list-buzz: ãƒã‚ºä¸€è¦§, import-csv: CSVã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ, sync: ã‚¹ãƒ—ã‚·åŒæœŸ, export: JSONå‡ºåŠ›")
    parser.add_argument("--limit", type=int, default=20, help="è¡¨ç¤ºä»¶æ•°")
    parser.add_argument("--spreadsheet-id", help="ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID")

    args = parser.parse_args()

    logger_instance = YouTubeSheetsLogger(args.spreadsheet_id)

    if args.command == "list-buzz":
        videos = logger_instance.get_buzz_videos(args.limit)
        print(f"ğŸ“Š ãƒã‚ºå‹•ç”» ({len(videos)}ä»¶):\n")
        for v in videos:
            print(f"  ğŸ”¥ PR={v.get('performance_ratio', 0):.1f}x | {v.get('view_count', 0):,}å†ç”Ÿ")
            print(f"     {v.get('title', '')[:50]}...")
            print(f"     {v.get('channel_name', '')} | {v.get('published_at', '')[:10]}")
            print()

    elif args.command == "import-csv":
        count = logger_instance.log_all_videos_from_csv()
        print(f"âœ… {count}ä»¶ã®å‹•ç”»ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ")

    elif args.command == "sync":
        # ãƒã‚ºå‹•ç”»
        buzz_params = logger_instance.sync_buzz_to_sheets()
        if "error" in buzz_params:
            print(f"âŒ {buzz_params['error']}")
        else:
            print("ğŸ“Š ãƒã‚ºå‹•ç”»åŒæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
            print(json.dumps(buzz_params, ensure_ascii=False, indent=2)[:500] + "...")

        # å…¨å‹•ç”»
        videos_params = logger_instance.sync_videos_to_sheets()
        if "error" not in videos_params:
            print(f"\nğŸ“Š å‹•ç”»ä¸€è¦§åŒæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {len(videos_params.get('data', []))}è¡Œ")

    elif args.command == "export":
        buzz = logger_instance.get_buzz_videos(1000)
        videos = logger_instance.get_all_videos()
        print(json.dumps({
            "buzz_videos": buzz,
            "all_videos": videos
        }, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
